{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract ScholarBERT Embeddings for materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"./known.pkl\"  # or \"./random.pkl\" or \"./relevant.pkl\"\n",
    "model_path = \"globuslabs/ScholarBERT-XL\"\n",
    "data_path = \"./\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Define utility functions for extracting contextualized BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_idx(sent: str, word: str, encoding):\n",
    "    sent = sent.lower()\n",
    "    word = word.lower()\n",
    "    start = sent.find(word)\n",
    "    end = start + len(word)\n",
    "    indices = list()\n",
    "    try:\n",
    "        for idx in range(len(sent)):\n",
    "            word_start, word_end = encoding.word_to_chars(idx)\n",
    "            if word_start >= end:\n",
    "                break\n",
    "            elif word_start >= start:\n",
    "                indices.append(idx)\n",
    "    except TypeError:\n",
    "        print(word, '\\t', sent)\n",
    "    return indices\n",
    "\n",
    "def get_hidden_states(encoded, token_ids_word, model, layers):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "    Select only those subword token outputs that belong to our word of interest\n",
    "    and average them.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded, output_hidden_states=True)\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word]\n",
    "    return word_tokens_output.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def get_word_vector(sent, word, tokenizer, model, layers):\n",
    "    \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n",
    "    that make up the word of interest, and then `get_hidden_states`.\"\"\"\n",
    "    encoded = tokenizer.encode_plus(sent, is_split_into_words=False, max_length=512, \n",
    "                                    truncation=True, return_tensors=\"pt\").to(device)\n",
    "    indices = get_word_idx(sent, word, encoded)\n",
    "    if indices:\n",
    "        # get all token idxs that belong to the word of interest\n",
    "        token_ids_word = np.where(np.isin(np.array(encoded.word_ids()), indices))\n",
    "        return get_hidden_states(encoded, token_ids_word, model, layers)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract contextualized embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(layers=None):\n",
    "    if torch.cuda.is_available():\n",
    "        print('Running on GPU')\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        print('Running on CPU')\n",
    "        device = 'cpu'\n",
    "\n",
    "    # Use last four layers by default\n",
    "    layers = [-4, -3, -2, -1] if layers is None else layers\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModel.from_pretrained(model_path, output_hidden_states=True)\n",
    "    model.to(device)\n",
    "\n",
    "    emb_dict = dict()\n",
    "    with open(data_path, 'r') as csvfile:\n",
    "        data_reader = csv.reader(csvfile)\n",
    "        # This skips the header row of the CSV file.\n",
    "        next(data_reader)\n",
    "        lines_read = 0\n",
    "        for filename, line, name, molecule, text in tqdm(data_reader):\n",
    "            word_embedding = get_word_vector(text, name, tokenizer, model, layers)\n",
    "            if word_embedding is not None:\n",
    "                if name not in emb_dict:\n",
    "                    emb_dict[name] = [1, word_embedding]\n",
    "                else:\n",
    "                    emb_dict[name][0] += 1\n",
    "                    emb_dict[name][1] += word_embedding\n",
    "            lines_read += 1\n",
    "        with open(out_file, 'wb') as fp_out:\n",
    "            pickle.dump(emb_dict, fp_out)\n",
    "    return emb_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('pytorch39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "022eef4a8379e749905971a853fd95b784ae657b59e2a838150e96e1a9109585"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
